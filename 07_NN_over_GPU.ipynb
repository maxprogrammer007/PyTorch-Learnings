{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x269ffb85030>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a rsandom seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>205</td>\n",
       "      <td>196</td>\n",
       "      <td>213</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>203</td>\n",
       "      <td>174</td>\n",
       "      <td>151</td>\n",
       "      <td>188</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      9       0       0       0       0       0       0       0       0   \n",
       "1      7       0       0       0       0       0       0       0       0   \n",
       "2      0       0       0       0       0       0       1       0       0   \n",
       "3      8       0       0       0       0       0       0       0       0   \n",
       "4      8       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         7         0        50       205       196   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...       142       142       142        21         0         3   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...       213       203       174       151       188        10   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       213       165         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fmnist_small.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "x = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the features\n",
    "\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, Features, Labels):\n",
    "        self.Features = torch.tensor(Features, dtype=torch.float32)\n",
    "        self.Labels = torch.tensor(Labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.Features[idx], self.Labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train_dataset object \n",
    "train_dataset = CustomDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test_dataset object \n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and test loader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN class\n",
    "\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate and epochs\n",
    "lr = 0.1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyNN(X_train.shape[1])\n",
    "model.to(device)\n",
    "\n",
    "# loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# using the best optimizer for the model - adam / sgd / rmsprop\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7689862847328186\n",
      "Epoch 2/100, Loss: 0.5638744235038757\n",
      "Epoch 3/100, Loss: 0.6357843279838562\n",
      "Epoch 4/100, Loss: 0.44449886679649353\n",
      "Epoch 5/100, Loss: 0.45776283740997314\n",
      "Epoch 6/100, Loss: 0.4718804657459259\n",
      "Epoch 7/100, Loss: 0.4849931597709656\n",
      "Epoch 8/100, Loss: 0.2724159359931946\n",
      "Epoch 9/100, Loss: 0.29381412267684937\n",
      "Epoch 10/100, Loss: 0.16522419452667236\n",
      "Epoch 11/100, Loss: 0.5025491714477539\n",
      "Epoch 12/100, Loss: 0.482719749212265\n",
      "Epoch 13/100, Loss: 0.616004467010498\n",
      "Epoch 14/100, Loss: 0.7262017726898193\n",
      "Epoch 15/100, Loss: 0.3375810980796814\n",
      "Epoch 16/100, Loss: 0.26656830310821533\n",
      "Epoch 17/100, Loss: 0.17142505943775177\n",
      "Epoch 18/100, Loss: 0.3644021451473236\n",
      "Epoch 19/100, Loss: 0.4625479578971863\n",
      "Epoch 20/100, Loss: 0.28775015473365784\n",
      "Epoch 21/100, Loss: 0.2589673399925232\n",
      "Epoch 22/100, Loss: 0.2872511148452759\n",
      "Epoch 23/100, Loss: 0.19640500843524933\n",
      "Epoch 24/100, Loss: 0.12837988138198853\n",
      "Epoch 25/100, Loss: 0.1472007781267166\n",
      "Epoch 26/100, Loss: 0.17158614099025726\n",
      "Epoch 27/100, Loss: 0.23263709247112274\n",
      "Epoch 28/100, Loss: 0.24673986434936523\n",
      "Epoch 29/100, Loss: 0.2149970382452011\n",
      "Epoch 30/100, Loss: 0.18216082453727722\n",
      "Epoch 31/100, Loss: 0.1786034256219864\n",
      "Epoch 32/100, Loss: 0.04587983340024948\n",
      "Epoch 33/100, Loss: 0.21570715308189392\n",
      "Epoch 34/100, Loss: 0.1796521097421646\n",
      "Epoch 35/100, Loss: 0.07906358689069748\n",
      "Epoch 36/100, Loss: 0.15062011778354645\n",
      "Epoch 37/100, Loss: 0.18772777915000916\n",
      "Epoch 38/100, Loss: 0.23926907777786255\n",
      "Epoch 39/100, Loss: 0.26063936948776245\n",
      "Epoch 40/100, Loss: 0.7756152153015137\n",
      "Epoch 41/100, Loss: 0.13555671274662018\n",
      "Epoch 42/100, Loss: 0.17590391635894775\n",
      "Epoch 43/100, Loss: 0.07733625918626785\n",
      "Epoch 44/100, Loss: 0.05073711648583412\n",
      "Epoch 45/100, Loss: 0.1975778043270111\n",
      "Epoch 46/100, Loss: 0.05701619014143944\n",
      "Epoch 47/100, Loss: 0.08751309663057327\n",
      "Epoch 48/100, Loss: 0.06466763466596603\n",
      "Epoch 49/100, Loss: 0.3406590521335602\n",
      "Epoch 50/100, Loss: 0.08502306044101715\n",
      "Epoch 51/100, Loss: 0.06809432804584503\n",
      "Epoch 52/100, Loss: 0.017115257680416107\n",
      "Epoch 53/100, Loss: 0.04238501191139221\n",
      "Epoch 54/100, Loss: 0.11472895741462708\n",
      "Epoch 55/100, Loss: 0.23607203364372253\n",
      "Epoch 56/100, Loss: 0.012640970759093761\n",
      "Epoch 57/100, Loss: 0.2366066724061966\n",
      "Epoch 58/100, Loss: 0.03366105630993843\n",
      "Epoch 59/100, Loss: 0.06377598643302917\n",
      "Epoch 60/100, Loss: 0.05633747950196266\n",
      "Epoch 61/100, Loss: 0.04520583152770996\n",
      "Epoch 62/100, Loss: 0.03655986115336418\n",
      "Epoch 63/100, Loss: 0.0658789724111557\n",
      "Epoch 64/100, Loss: 0.03375718742609024\n",
      "Epoch 65/100, Loss: 0.06138207018375397\n",
      "Epoch 66/100, Loss: 0.008040891028940678\n",
      "Epoch 67/100, Loss: 0.03513873368501663\n",
      "Epoch 68/100, Loss: 0.029845641925930977\n",
      "Epoch 69/100, Loss: 0.03741120919585228\n",
      "Epoch 70/100, Loss: 0.0031147266272455454\n",
      "Epoch 71/100, Loss: 0.09990597516298294\n",
      "Epoch 72/100, Loss: 0.056056417524814606\n",
      "Epoch 73/100, Loss: 0.03723696991801262\n",
      "Epoch 74/100, Loss: 0.031049611046910286\n",
      "Epoch 75/100, Loss: 0.025468235835433006\n",
      "Epoch 76/100, Loss: 0.11319536715745926\n",
      "Epoch 77/100, Loss: 0.09510976076126099\n",
      "Epoch 78/100, Loss: 0.02438531070947647\n",
      "Epoch 79/100, Loss: 0.009462089277803898\n",
      "Epoch 80/100, Loss: 0.05045488476753235\n",
      "Epoch 81/100, Loss: 0.11770696938037872\n",
      "Epoch 82/100, Loss: 0.013726156204938889\n",
      "Epoch 83/100, Loss: 0.021571367979049683\n",
      "Epoch 84/100, Loss: 0.004649918060749769\n",
      "Epoch 85/100, Loss: 0.0076809655874967575\n",
      "Epoch 86/100, Loss: 0.01026404369622469\n",
      "Epoch 87/100, Loss: 0.028139011934399605\n",
      "Epoch 88/100, Loss: 0.005594711285084486\n",
      "Epoch 89/100, Loss: 0.10074087232351303\n",
      "Epoch 90/100, Loss: 0.0027730651199817657\n",
      "Epoch 91/100, Loss: 0.007770316209644079\n",
      "Epoch 92/100, Loss: 0.004999433644115925\n",
      "Epoch 93/100, Loss: 0.003786463988944888\n",
      "Epoch 94/100, Loss: 0.12612088024616241\n",
      "Epoch 95/100, Loss: 0.0005757673061452806\n",
      "Epoch 96/100, Loss: 0.19271662831306458\n",
      "Epoch 97/100, Loss: 0.00537616154178977\n",
      "Epoch 98/100, Loss: 0.011003980413079262\n",
      "Epoch 99/100, Loss: 0.025934437289834023\n",
      "Epoch 100/100, Loss: 0.016547061502933502\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output = model.train()\n",
    "    for features, labels in train_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in test_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {correct/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | Sequential       | 109 K  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abhin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [00:01<00:00, 93.39it/s, v_num=1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 150/150 [00:01<00:00, 92.40it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 38/38 [00:00<00:00, 130.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8308333158493042     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2794222831726074     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8308333158493042    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2794222831726074    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.2794222831726074, 'test_accuracy': 0.8308333158493042}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('fmnist_small.csv')\n",
    "\n",
    "# Train-test split\n",
    "x = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Create a CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, Features, Labels):\n",
    "        self.Features = torch.tensor(Features, dtype=torch.float32)\n",
    "        self.Labels = torch.tensor(Labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.Features[idx], self.Labels[idx]\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Define the Neural Network Model\n",
    "class MyNN(pl.LightningModule):\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, labels = batch\n",
    "        outputs = self(features)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, labels = batch\n",
    "        outputs = self(features)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == labels).float().mean()\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', accuracy)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        features, labels = batch\n",
    "        outputs = self(features)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', accuracy)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Initialize the model\n",
    "model = MyNN(input_size=X_train.shape[1])\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = Trainer(max_epochs=100, accelerator='gpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
